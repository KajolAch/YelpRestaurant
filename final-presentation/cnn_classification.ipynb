{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413\n",
      "Average length of sentences is 90\n",
      "91.57869249394673\n",
      "['Ambience' 'Cleanliness' 'Food' 'None' 'Service']\n",
      "[ 65  45 365  17 212]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 100)     750100      main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_unigram (Conv1D)           (None, 100, 32)      3232        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_bigram (Conv1D)            (None, 99, 32)       6432        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_trigram (Conv1D)           (None, 98, 32)       9632        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pool_unigram (MaxPooling1D)     (None, 1, 32)        0           conv_unigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "pool_bigram (MaxPooling1D)      (None, 1, 32)        0           conv_bigram[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "pool_trigram (MaxPooling1D)     (None, 1, 32)        0           conv_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat_unigram (Flatten)          (None, 32)           0           pool_unigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat_bigram (Flatten)           (None, 32)           0           pool_bigram[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_trigram (Flatten)          (None, 32)           0           pool_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concate (Concatenate)           (None, 96)           0           flat_unigram[0][0]               \n",
      "                                                                 flat_bigram[0][0]                \n",
      "                                                                 flat_trigram[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 96)           0           concate[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 96)           9312        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 5)            485         dense_28[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 779,193\n",
      "Trainable params: 779,193\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train on 320 samples, validate on 50 samples\n",
      "Epoch 1/50\n",
      "320/320 [==============================] - 4s 12ms/step - loss: 0.6667 - acc: 0.6531 - val_loss: 0.5104 - val_acc: 0.7280\n",
      "Epoch 2/50\n",
      "320/320 [==============================] - 0s 933us/step - loss: 0.6022 - acc: 0.6531 - val_loss: 0.4277 - val_acc: 0.7280\n",
      "Epoch 3/50\n",
      "320/320 [==============================] - 0s 951us/step - loss: 0.5448 - acc: 0.6825 - val_loss: 0.3484 - val_acc: 0.8600\n",
      "Epoch 4/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.5227 - acc: 0.7738 - val_loss: 0.3037 - val_acc: 0.9120\n",
      "Epoch 5/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.5170 - acc: 0.7900 - val_loss: 0.2978 - val_acc: 0.9120\n",
      "Epoch 6/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.5124 - acc: 0.7919 - val_loss: 0.3123 - val_acc: 0.9120\n",
      "Epoch 7/50\n",
      "320/320 [==============================] - 0s 923us/step - loss: 0.4965 - acc: 0.7869 - val_loss: 0.3231 - val_acc: 0.9040\n",
      "Epoch 8/50\n",
      "320/320 [==============================] - 0s 917us/step - loss: 0.4919 - acc: 0.7738 - val_loss: 0.3274 - val_acc: 0.8920\n",
      "Epoch 9/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4820 - acc: 0.7806 - val_loss: 0.3229 - val_acc: 0.9040\n",
      "Epoch 10/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4803 - acc: 0.7875 - val_loss: 0.3192 - val_acc: 0.9120\n",
      "Epoch 11/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4737 - acc: 0.7938 - val_loss: 0.3159 - val_acc: 0.9080\n",
      "Epoch 12/50\n",
      "320/320 [==============================] - 0s 880us/step - loss: 0.4598 - acc: 0.7844 - val_loss: 0.3157 - val_acc: 0.9080\n",
      "Epoch 13/50\n",
      "320/320 [==============================] - 0s 989us/step - loss: 0.4552 - acc: 0.7819 - val_loss: 0.3102 - val_acc: 0.9080\n",
      "Epoch 14/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4484 - acc: 0.7825 - val_loss: 0.3083 - val_acc: 0.9040\n",
      "Epoch 15/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.4359 - acc: 0.7981 - val_loss: 0.3029 - val_acc: 0.9080\n",
      "Epoch 16/50\n",
      "320/320 [==============================] - 0s 914us/step - loss: 0.4291 - acc: 0.7887 - val_loss: 0.2981 - val_acc: 0.9080\n",
      "Epoch 17/50\n",
      "320/320 [==============================] - 0s 931us/step - loss: 0.4155 - acc: 0.7894 - val_loss: 0.3010 - val_acc: 0.9040\n",
      "Epoch 18/50\n",
      "320/320 [==============================] - 0s 877us/step - loss: 0.4062 - acc: 0.7944 - val_loss: 0.2967 - val_acc: 0.9080\n",
      "Epoch 19/50\n",
      "320/320 [==============================] - 0s 2ms/step - loss: 0.3969 - acc: 0.7950 - val_loss: 0.2896 - val_acc: 0.9080\n",
      "Epoch 20/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3858 - acc: 0.7994 - val_loss: 0.2903 - val_acc: 0.9080\n",
      "Epoch 21/50\n",
      "320/320 [==============================] - 0s 836us/step - loss: 0.3740 - acc: 0.7969 - val_loss: 0.2946 - val_acc: 0.9080\n",
      "Epoch 22/50\n",
      "320/320 [==============================] - 0s 938us/step - loss: 0.3657 - acc: 0.7931 - val_loss: 0.2884 - val_acc: 0.9080\n",
      "Epoch 23/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3529 - acc: 0.7956 - val_loss: 0.2839 - val_acc: 0.9080\n",
      "Epoch 24/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.3512 - acc: 0.8075 - val_loss: 0.2848 - val_acc: 0.9080\n",
      "Epoch 25/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3511 - acc: 0.8031 - val_loss: 0.2868 - val_acc: 0.9080\n",
      "Epoch 26/50\n",
      "320/320 [==============================] - 0s 952us/step - loss: 0.3347 - acc: 0.8050 - val_loss: 0.2854 - val_acc: 0.9080\n",
      "Epoch 27/50\n",
      "320/320 [==============================] - 0s 910us/step - loss: 0.3399 - acc: 0.8056 - val_loss: 0.2848 - val_acc: 0.9080\n",
      "Epoch 28/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3306 - acc: 0.8094 - val_loss: 0.2825 - val_acc: 0.9080\n",
      "Epoch 29/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3272 - acc: 0.7994 - val_loss: 0.2829 - val_acc: 0.9080\n",
      "Epoch 30/50\n",
      "320/320 [==============================] - 0s 979us/step - loss: 0.3223 - acc: 0.8050 - val_loss: 0.2874 - val_acc: 0.9080\n",
      "Epoch 31/50\n",
      "320/320 [==============================] - 0s 959us/step - loss: 0.3149 - acc: 0.8094 - val_loss: 0.2830 - val_acc: 0.9080\n",
      "Epoch 32/50\n",
      "320/320 [==============================] - 0s 967us/step - loss: 0.3124 - acc: 0.8125 - val_loss: 0.2841 - val_acc: 0.9080\n",
      "Epoch 33/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.3162 - acc: 0.8138 - val_loss: 0.2874 - val_acc: 0.9080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.3101 - acc: 0.8138 - val_loss: 0.2858 - val_acc: 0.9080\n",
      "Epoch 35/50\n",
      "320/320 [==============================] - 0s 822us/step - loss: 0.3053 - acc: 0.8125 - val_loss: 0.2861 - val_acc: 0.9080\n",
      "Epoch 36/50\n",
      "320/320 [==============================] - 0s 908us/step - loss: 0.3067 - acc: 0.8119 - val_loss: 0.2868 - val_acc: 0.9080\n",
      "Epoch 37/50\n",
      "320/320 [==============================] - 0s 790us/step - loss: 0.3004 - acc: 0.8175 - val_loss: 0.2874 - val_acc: 0.9080\n",
      "Epoch 38/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2986 - acc: 0.8150 - val_loss: 0.2885 - val_acc: 0.9080\n",
      "Epoch 39/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2948 - acc: 0.8150 - val_loss: 0.2874 - val_acc: 0.9080\n",
      "Epoch 40/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.2905 - acc: 0.8144 - val_loss: 0.2884 - val_acc: 0.9080\n",
      "Epoch 41/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.2885 - acc: 0.8156 - val_loss: 0.2900 - val_acc: 0.9080\n",
      "Epoch 42/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.2861 - acc: 0.8131 - val_loss: 0.2920 - val_acc: 0.9080\n",
      "Epoch 43/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2842 - acc: 0.8194 - val_loss: 0.2939 - val_acc: 0.9080\n",
      "Epoch 44/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.2895 - acc: 0.8200 - val_loss: 0.2947 - val_acc: 0.9080\n",
      "Epoch 45/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2809 - acc: 0.8175 - val_loss: 0.2956 - val_acc: 0.9080\n",
      "Epoch 46/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2797 - acc: 0.8188 - val_loss: 0.2973 - val_acc: 0.9080\n",
      "Epoch 47/50\n",
      "320/320 [==============================] - 0s 898us/step - loss: 0.2785 - acc: 0.8225 - val_loss: 0.3005 - val_acc: 0.9080\n",
      "Epoch 48/50\n",
      "320/320 [==============================] - 1s 2ms/step - loss: 0.2745 - acc: 0.8213 - val_loss: 0.3014 - val_acc: 0.9080\n",
      "Epoch 49/50\n",
      "320/320 [==============================] - 0s 1ms/step - loss: 0.2710 - acc: 0.8244 - val_loss: 0.3029 - val_acc: 0.9040\n",
      "Epoch 50/50\n",
      "320/320 [==============================] - 0s 878us/step - loss: 0.2739 - acc: 0.8225 - val_loss: 0.3041 - val_acc: 0.9080\n",
      "acc: 79.53%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from numpy.random import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, Conv1D, MaxPooling1D, \\\n",
    "Dropout, Activation, Input, Flatten, Concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "MAX_NB_WORDS = 7500\n",
    "MAX_DOC_LEN = 100\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "\n",
    "def find_avg_wordlen(text):\n",
    "    print(\"Average length of sentences is 90\")\n",
    "    a = np.asarray(text)\n",
    "    c = np.array([])\n",
    "    for i in a:\n",
    "        b = len(i.split())\n",
    "        c = np.append(c, b)\n",
    "    print(np.mean(c))\n",
    "\n",
    "\n",
    "def build_tokenizer(text):\n",
    "    # documents are quite long in the dataset\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    voc = tokenizer.word_index\n",
    "    # convert each document to a list of word index as a sequence\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    # get the mapping between words to word index\n",
    "\n",
    "    # pad all sequences into the same length (the longest)\n",
    "    padded_sequences = pad_sequences(sequences, \\\n",
    "                                     maxlen=MAX_DOC_LEN, \\\n",
    "                                     padding='post', truncating='post')\n",
    "\n",
    "    # print(padded_sequences[6])\n",
    "    return padded_sequences\n",
    "\n",
    "def build_model():\n",
    "\n",
    "    # define input layer, where a sentence represented as\n",
    "    # 1 dimension array with integers\n",
    "    main_input = Input(shape=(MAX_DOC_LEN,), dtype='int32', name='main_input')\n",
    "\n",
    "    # define the embedding layer\n",
    "    # input_dim is the size of all words +1\n",
    "    # where 1 is for the padding symbol\n",
    "    # output_dim is the word vector dimension\n",
    "    # input_length is the max. length of a document\n",
    "    # input to embedding layer is the \"main_input\" layer\n",
    "    embed_1 = Embedding(input_dim=MAX_NB_WORDS + 1,\n",
    "                        output_dim=EMBEDDING_DIM,\n",
    "                        input_length=MAX_DOC_LEN,\n",
    "                        name='embedding')(main_input)\n",
    "\n",
    "    conv1d_1 = Conv1D(filters=32, kernel_size=1,\n",
    "                      name='conv_unigram',\n",
    "                      activation='relu')(embed_1)\n",
    "\n",
    "    pool_1 = MaxPooling1D(MAX_DOC_LEN - 1 + 1, \\\n",
    "                          name='pool_unigram')(conv1d_1)\n",
    "\n",
    "    flat_1 = Flatten(name='flat_unigram')(pool_1)\n",
    "\n",
    "    conv1d_2 = Conv1D(filters=32, kernel_size=2, \\\n",
    "                      name='conv_bigram', \\\n",
    "                      activation='relu')(embed_1)\n",
    "    pool_2 = MaxPooling1D(MAX_DOC_LEN - 2 + 1, name='pool_bigram')(conv1d_2)\n",
    "    flat_2 = Flatten(name='flat_bigram')(pool_2)\n",
    "\n",
    "    conv1d_3 = Conv1D(filters=32, kernel_size=3, \\\n",
    "                      name='conv_trigram', activation='relu')(embed_1)\n",
    "    pool_3 = MaxPooling1D(MAX_DOC_LEN - 3 + 1, name='pool_trigram')(conv1d_3)\n",
    "    flat_3 = Flatten(name='flat_trigram')(pool_3)\n",
    "\n",
    "    z = Concatenate(name='concate')([flat_1, flat_2, flat_3])\n",
    "\n",
    "    # Create a dropout layer\n",
    "    # In each iteration only 50% units are turned on\n",
    "    drop_1 = Dropout(rate=0.5, name='dropout')(z)\n",
    "\n",
    "    # Create a dense layer\n",
    "    dense_1 = Dense(96, activation='relu')(drop_1)\n",
    "#     drop_1 = Dropout(rate=0.5)(dense_1)\n",
    "#     dense_1 = Dense(100, activation='relu')(drop_1)\n",
    "#     dense_1 = Dense(100, activation='relu')(dense_1)\n",
    "    \n",
    "#     dense_1 = Dense(128, activation='relu')(dense_1)\n",
    "    # Create the output layer\n",
    "    preds = Dense(5, activation='softmax', name='output')(dense_1)\n",
    "\n",
    "    # create the model with input layer\n",
    "    # and the output layer\n",
    "    model = Model(inputs=main_input, outputs=preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"labeled_il_reviews_large.csv\", header = 0)\n",
    "    # print(semEval_df.head())\n",
    "\n",
    "    text = df['text'].values\n",
    "    tags = df['tags'].values\n",
    "    print(len(text))\n",
    "    labels = []\n",
    "\n",
    "    for x in tags:\n",
    "        tag = x.split(\", \")   # tags are separated by comma and space. Eg, Cleanliness, Food\n",
    "        labels.append(tag)\n",
    "\n",
    "    data = list(zip(*(text, labels)))\n",
    "\n",
    "    text = [x[0] for x in data]\n",
    "    find_avg_wordlen(text)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    Y = mlb.fit_transform(labels)\n",
    "    print(mlb.classes_)\n",
    "    print(np.sum(Y, axis=0))\n",
    "\n",
    "    padded_sequences = build_tokenizer(text)\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "    #                         padded_sequences, Y,\\\n",
    "    #                         test_size=0.3, random_state=1)\n",
    "    \n",
    "    X_train = padded_sequences[:320]\n",
    "    X_val = padded_sequences[320:370]\n",
    "    X_test = padded_sequences[370:]\n",
    "    \n",
    "    y_train = Y[:320]\n",
    "    y_val = Y[320:370]\n",
    "    y_test = Y[370:]\n",
    "    \n",
    "    cnn_model = build_model()\n",
    "    print(cnn_model.summary())\n",
    "\n",
    "    cnn_model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=\"adam\",\n",
    "                      metrics=[\"accuracy\"])\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHES = 50\n",
    "\n",
    "    # fit the model and save fitting history to \"training\"\n",
    "    training = cnn_model.fit(X_train, y_train, \\\n",
    "                         batch_size=BATCH_SIZE, \\\n",
    "                         epochs=NUM_EPOCHES, \\\n",
    "                         validation_data=[X_val, y_val])\n",
    "    \n",
    "    pred=cnn_model.predict(X_test)\n",
    "    # evaluate the model\n",
    "    scores = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (cnn_model.metrics_names[1], scores[1]*100))\n",
    "    \n",
    "#     df = pd.DataFrame.from_dict(training.history)\n",
    "#     df.columns = [\"train_acc\", \"train_loss\", \\\n",
    "#                   \"val_acc\", \"val_loss\"]\n",
    "#     df.index.name = 'epoch'\n",
    "#     print(df)\n",
    "\n",
    "#     # plot training history\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3));\n",
    "\n",
    "#     df[[\"train_acc\", \"val_acc\"]].plot(ax=axes[0]);\n",
    "#     df[[\"train_loss\", \"val_loss\"]].plot(ax=axes[1]);\n",
    "#     plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-7e4c180e3573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0;36m9.9030335e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8.0543815e-04\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.4468492e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.9199455e-02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.9802197e-04\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    3.2074266e-05, 4.2012878e-05, 6.2628539e-04, 3.6239010e-04, 4.7610620e-06]]]\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "p = [[[9.6844585e-04, 3.6410633e-02, 1.6466923e-03, 9.5035657e-03, 3.2031160e-02,\n",
    "   4.1820608e-02, 2.0942863e-03, 2.9957650e-04, 3.8437967e-03, 8.0282564e-07,\n",
    "   7.2959522e-03, 5.0763118e-01, 5.2764505e-02, 5.1746462e-03, 3.1874825e-03,\n",
    "   2.2540364e-01, 6.6833437e-04, 2.0476615e-04, 3.2938635e-03, 3.3450838e-02,\n",
    "   9.9030335e-04, 8.0543815e-04, 4.4468492e-05, 2.9199455e-02, 1.9802197e-04,\n",
    "   3.2074266e-05, 4.2012878e-05, 6.2628539e-04, 3.6239010e-04, 4.7610620e-06]]]\n",
    "p = p[0, 0,:]\n",
    "p = np.power(p, (1/0.5))\n",
    "p = p/np.sum(p)\n",
    "p = p[0,0,:]\n",
    "prob = np.random.multinomial(1, p, 1)\n",
    "print(p)\n",
    "print(prob)\n",
    "print(np.argmax(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = 0.5\n",
    "a = [[[9.6844585e-04, 3.6410633e-02, 1.6466923e-03, 9.5035657e-03, 3.2031160e-02,\n",
    "   4.1820608e-02, 2.0942863e-03, 2.9957650e-04, 3.8437967e-03, 8.0282564e-07,\n",
    "   7.2959522e-03, 5.0763118e-01, 5.2764505e-02, 5.1746462e-03, 3.1874825e-03,\n",
    "   2.2540364e-01, 6.6833437e-04, 2.0476615e-04, 3.2938635e-03, 3.3450838e-02,\n",
    "   9.9030335e-04, 8.0543815e-04, 4.4468492e-05, 2.9199455e-02, 1.9802197e-04,\n",
    "   3.2074266e-05, 4.2012878e-05, 6.2628539e-04, 3.6239010e-04, 4.7610620e-06]]]\n",
    "a = np.log(a) / temperature \n",
    "a = a[0,0,:]\n",
    "dist = np.exp(a)/np.sum(np.exp(a)) \n",
    "choices = range(len(a)) \n",
    "np.random.choice(choices, p=dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
